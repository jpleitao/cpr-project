\documentclass[9pt,journal,compsoc]{IEEEtran}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{array}
\usepackage{algorithmic}
\usepackage{url}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\newtheorem{mydef}{Definition}

\begin{document}

\title{\huge Categorisation of Urban Water Consumptions}

% Author information
\author{
	\IEEEauthorblockN{Joaquim Leitão\IEEEauthorrefmark{1}}
	
	\IEEEauthorblockA{\IEEEauthorrefmark{1}jpleitao@dei.uc.pt \\ CISUC, Department of Informatics Engineering, Univesity of Coimbra, Portugal}
}

% The paper headers
\markboth{Connectivity and Pattern Recognition, 2016-2017}{Connectivity and Pattern Recognition, 2016-2017}


\IEEEtitleabstractindextext{
\begin{abstract}
	Lol, here goes the abstract
\end{abstract}

\begin{IEEEkeywords}
	Water Consumption Patterns, Clustering, Time series, Pattern Recognition.
\end{IEEEkeywords}
}


% make the title area
\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle


\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\IEEEPARstart{I}{n} most cities and urban areas investment in equipment related to water supply and drainage is substantial, being responsible for a considerable share of the budget of many municipalities.

Substantial attention has been given to the construction and installation of new pipe and sewage infrastructures, as well as to maintenance operations on existing infrastructures; however, relatively small efforts have been made to study water consumption patterns in these environments.

Indeed, understanding and accurately predicting such behaviours is extremely important to water companies, as it allows them to improve the management of their infrastructures, namely water storage facilities (such as water tanks and towers): Many water utilities repeatedly overestimate the volume of water needed to supply the population of a given region. Therefore, water companies tend to operate existing water storage tanks close to their full capacity, resulting in higher energy-related costs (since water has to be pumped into the tanks) and in larger volumes of retained water. 

By being capable of characterising water consumption patterns in a given region water companies could adjust water volumes in existing storage tanks\footnote{For obvious reasons this volume must always be overestimated; however, such overestimation could be better controlled.}, improving the management of their finances and reducing excess water volumes retained in storage tanks.

The current work aims at categorising water consumptions in a given region of a median size city, contributing towards an improved understanding and knowledge of water consumption patterns. Therefore, in this work, historic water consumptions are intended to be processed and organised in different groups according to their similarity.

In the field of pattern recognition, tasks of this nature are referred to as \emph{clustering} approaches. Since a time-ordered series of data (water consumptions, in this case) is intended to be processed and clustered, the proposed problem can be considered as \emph{time series clustering}. In this sense, it is important to define this term. We present a definition proposed by Aghabozorgi \emph{et al.} \cite{aghabozorgi2015time}:

\begin{mydef}[Time Series Clustering]
	"Given a dataset of n time series data $D = \{F_{1}, \cdots , F_{n}\}$, time series clustering can be defined as the process of unsupervised partitioning of $D$ into $C = {C_{1}, \cdots , C_{k}}$ in such a way that homogeneous time series are grouped together based on a certain similarity measure."
\end{mydef}

The remainder of this document is organised as follows: Section \ref{objectives} presents the main objectives of this work. In section \ref{dataset_description} water consumption data used in this work is briefly presented. The methodology adopted throughout this work is discussed in section \ref{methodology} and an overview of the experimental setup is provided in section \ref{experiments_summary}. Sections \ref{pre_processing} to \ref{results_assessment} cover the main steps of the work, presented in the methodology section. Finally, section \ref{conclusions} concludes this document.

\textbf{FIXME: Review outline}

\section{Objectives}
\label{objectives}

The main objective of the proposed work is related with the categorisation of urban domestic water consumptions in a given region of a median size city.

That is, taking into account information about an entire civil year (from January 1st to December 31st) the system must be capable of identifying recurrent consumption patterns and determine the moments in time when such patterns were repeated: an intuitive (and somewhat expected) result is that water consumptions during summer months will have a similar shape, which substantially differs from those recorded during winter months. This could be justified by the fact that many people in that region usually spend summer months away, on vacation.

In this sense, the appropriate steps of a pattern recognition system will be considered, namely: (i) data pre-processing; (ii) pattern recognition techniques - in this case clustering techniques; (iii) analysis of the experimental results.

\section{Dataset Description}
\label{dataset_description}

The dataset to be used in the proposed project contains domestic water consumptions in different regions of a median size city. The available data corresponds to the entire 2016 year and was obtained as a result of a collaboration with \emph{Águas de Coimbra}\footnote{\url{http://www.aguasdecoimbra.pt/}}, the public water company in the city of Coimbra, Portgual.

Water consumption's measurement is carried out in strategic locations of the city, through measurement and control zones - \emph{ZMC}\footnote{Or in portuguese, \emph{Zonas de Medição e Controlo}.}. These infrastructures provide a more efficient management of the water distribution systems, enabling the network's management in logical zones of analysis and allowing the adoption of measures to control water losses.

Collected data corresponds to a time series of total water volume distributed to the region in question - recorded as a floating point number whose units are $m^{3}/h$. Therefore, pairs of values $(time, volume)$ are provided. The collection time is provided in the format \emph{dd/mm/yyyy HH:MM:SS}.

During the data acquisition equipment errors and malfunctions can occur, resulting in the failure to record the total consumptions. In such moments, a value of \emph{"n/a"} is presented for the total distributed water volume, signalling the missing value. The developed pattern recognition system must adopt proper techniques to deal with these missing values.

\section{Methodology}
\label{methodology}

Extensive research has been made in the field of time series clustering, resulting in the adoption and proposal of a wide variety of clustering approaches and techniques. In a 2015 survey on time series clustering, Aghabozorgi \emph{et al.} \cite{aghabozorgi2015time} highlights techniques such as \emph{hierarchical clustering}, \emph{k-means clustering} and \emph{k-medoids clustering}. Other techniques have also been applied, such as neural networks and variations of the mentioned algorithms, namely of \emph{k-means} ans \emph{k-medoids}.

Determining the most appropriate technique to be applied in our problem is far from being an easy task. The same methods applied on data of distinct natures can produce completely different results, and the same can even occur with similar data, collected from distinct sources and (most probably) with a different process.

As a result, in the current work, different techniques are intended to be explored in an attempt to determine the ones that appear to be more suitable to the collected data. These techniques fall in four distinct categories that compose four main steps of time series clustering. In subsequent sections a more derailed overview of such categories will be provided.

\begin{enumerate}
	\item \textbf{Time Series pre-processing}, comprising two major tasks: computing the unit of analysis for the time series; and imputing missing values
	
	\item \textbf{Time Series Representation}, consisting in the application of dimensionality reduction techniques. As will be explained later in this section, both raw time series data and dimensionality reduced time series data is intended to be clustered, in order to compare the results obtained with both methods.
	
	\item \textbf{Time Series Segmentation}, consisting in the application of clustering techniques to group similar samples together.
	
	\item \textbf{Results analysis and assessment}
\end{enumerate}
 
Regarding missing data imputation, three distinct techniques are considered to be applied to provide sounding estimations for missing water consumptions data: fitting a polynomial expression, fitting an ARIMA model or the application of Kalman Filters.

With respect to time series representation methods, two alternatives are intended to be considered: on one side, the popular \emph{raw time series} representation - where time series are represented as an ordered list of values collected over time; as an alternative, the applications of dimensionality reduction methods via \emph{Principal Components Analysis} and \emph{Stacked Autoencoders} is also explored.
	
Several clustering algorithms have been proposed and studied in the literature; however, based on the studied and referenced works, more traditional algorithms such as \emph{Hierarchical} and \emph{K-Means} clustering still remain popular and valid choices among researchers in this field. In this sense, their performance when applied to our data will be assessed. Another algorithm applied in time series clustering problems will be implemented and evaluated: this is the case of the recently proposed \emph{TADPole} density-based clustering algorithm, featuring a pruning strategy that softens the computational burden of the application of traditional clustering algorithms to time series.

\section{Time Series Pre-Processing}
\label{pre_processing}

As stated in section \ref{methodology}, time series pre-processing comprises two main iterations: computing the unit of analysis of the time series and imputing missing values in time series' readings. These steps will be covered in more detail in their own subsections.

\subsection{Unit of Analysis}

When identifying patterns of recurrent behaviour in time series data it is important to determine the periods of time with the higher contributions to the overall signal. In other words, the most intense and important periods of time are sought.

Similarly to the works of Abreu \emph{et al.}\cite{abreu2012using} and Calabrese \emph{et al.}\cite{calabrese2010eigenplaces}, a \emph{Fourier} analysis can be conducted on the collected time series data to determine the frequencies with higher contribution. In this sense, a \emph{Fast Fourier  Transform} (FFT) was performed on the time series data. 

Analysing the results of such transformation, it was determined that the frequency with the largest contribution to the resulting signal was the frequency 0 $Hz$. Even though this may seem an unexpected result, a valid explanation can be found: by performing the FFT of a given signal, the frequency with higher contribution is usually an approximation of the fundamental frequency. As the signal being analysed (the time series data) is not periodic, its fundamental period is infinite, rendering its fundamental frequency to be 0 $Hz$. In such scenarios, the second frequency with higher contribution is usually considered as the unit of analysis. In this case such a frequency was around $24h$, suggesting this to be the unit of analysis.

With this finding, an aggregation of the time series values was conducted: the values of the original time series were recorded with a one-minute interval; since the unit of analysis is $24h$, each sample to be clustered contains readings of one day, with an interval of $1h$\footnote{Therefore, each sample consists in a list of 24 values, one per hour. These individual hourly consumptions were obtained simply by adding all partial consumptions in the same hour.}.

\subsection{Missing Data Imputation}

In almost all real-life applications, errors in the data collection procedures are prompted to occur, forcing data analysis and processing methods to properly deal with missing and invalid values. Substantial research has been performed on the topic of missing data imputation, resulting in different methods and techniques being proposed and tested in a variety of problems.

Nevertheless, choosing a valid missing data imputation technique to be applied in a given problem is far from being an easy task. The same method can have completely distinct performances when applied to different data, of diverse natures and/or collected in different ways. In the particular case of this work, no previous studies of the performance of missing data imputation methods had been conducted on the collected data. As a result, a choice was made to browse the literature on this topic and test the application of the most cited and used ones in the collected data.

Steffen \emph{et al.}\cite{moritz2015comparison} studied the application of different missing data imputation methods on univariate time series. In their work, the authors obtained interesting results for approaches exploring linear interpolation, ARIMA and SARIMA models and Kalman Filters. 

Alternative methods can also be considered. Usually more active in time series prediction and classification, techniques featuring artificial neural networks, decision tress or k-nearest neighbours\cite{troyanskaya2001missing, yu2014short} can also be applied to compute estimations for missing values. In addition, traditional time series analysis methods such as autocorrelation and trend analysis have also been applied\cite{abreu2012using}, as well as expectation-maximization and multivariate algorithms on lagged data\cite{moritz2015comparison}.

Inspired by the results reported on the literature, and after an initial study of the different methods, a choice was made to select the following three missing data imputation methods for further analysis: (i) linear interpolation; (ii) ARIMA; (iii) Kalman Filter.

At this point, the goal of this analysis was to studying the performance of these methods when applied to the collected data. This study was conducted as follows: Initially individual samples were divided into samples that contained missing values and samples without missing value\footnote{The reader must keep in mind that individual samples consist in a list with water consumptions for a given day, featuring hourly values.}. For each complete sample missing values were artificially introduced. Each method was then applied to the individual samples and the correctness of their imputations was assessed using the \emph{root mean squared error} (rmse) metric.

Overall the Kalman Filter produced better estimates than the competing alternatives: An average rmse of about $263.83$ was registered (against a rmse of about $571.37$ for the ARIMA model and $12403.17$ for the linear interpolation), and the Kalman filter produced better daily estimates (smaller rmse on individual samples) in 189 days (against 72 for the ARIMA model and only 4 for the linear interpolation).

Based on such results the Kalman Filter was chosen as the imputation approach for our data. Therefore, for each day where missing data were registered, a Kalman Filter model was computed and estimations for the missing data in that day were obtained.

\section{Time Series Representation}
\label{time_series_representation}

When working with high dimensional data, computational requirements grow both in terms of memory and execution time: if each time series contains more values then it will require more space in memory and, therefore, if a larger number of time series is intended to be clustered or processed in some way substantially more memory may be required to load all the time series data. In addition, specially in the case of clustering, distance computation between data samples is often performed; if a larger number of samples is considered, each comprising several dimensions, distance computation between pairs of samples will take longer to compute, as will the overall process.

Another motivation for seeking alternative time series representations, as pointed out by Aghabozorgi \emph{et al.}\cite{aghabozorgi2015time}, has to do with the fact that when measuring the distance between raw time series samples highly unintuitive results may be gathered resulting in the clustering of series similar in noise, instead of shape. It must be noticed that the adopted distance metric highly influences the clustering results.

In this sense, several time series representation methods have been proposed in the literature. Aghabozorgi \emph{et al.} presents a taxonomy comprising four main categories: i) Data adaptive; (ii) Non-data adaptive; (iii) Model-based; and (iv) Data dictated. The choice of time series representation has a big impact in the outcome of the clustering exercise, in both quality and execution time. Furthermore, time series representation also affects the choice of distance metric (also called dissimilarity function).

In the specific clustering problem being tackled in this work, where time series containing similar behaviours are intended to be grouped together, one of the major challenges to be face has to due with the dissimilarity function (that is, how to determine that two time series are close together). Even though it is computationally expensive, the \emph{Dynamic Time Warping} (DTW)\cite{chu2002iterative} distance metric is quite popular, having been extensively adopted when dealing with raw time series data\cite{liao2005clustering, petitjean2014dynamic, izakian2015fuzzy, aghabozorgi2015time} and being considered a more robust distance metric for time series\cite{wang2013experimental}. 

As the DTW targets time sequences, it can be concluded that a substantial number of approaches adopt a raw time series representation. Dimensionality Reduction (DR) techniques such as \emph{Principal Components Analysis} (PCA) have also been explored \cite{abreu2012using} in an attempt to represent time series samples in fewer dimensions, thus reducing the computational burden of distance computation. As the application of DR techniques loses the temporal sequence of the data, alternative distance metrics need to be considered.

In light of the discussion carried out in this section, in the scope of the current work, it is intended to explore clustering performance obtained using two distinct time series representation approaches: (i) \textbf{Raw time series representation}, featuring the use of the popular \emph{Dynamic Time Warping} distance metric; (ii) \textbf{"Dimensionality-Reduced" time series representation}, featuring the use of the classical \emph{Euclidean} distance.

\subsection{Raw Time Series Representation}

With respect to the \emph{raw} time series representation not much needs to be added to what has already been mentioned. Initially, this representation can be seen as considering raw collected time series values.

Taking into account the pre-processing steps mentioned in section \ref{pre_processing}, imputed time series corresponding to daily lists of hourly water consumption readings should be considered at this point; however, an additional step must be considered: when working with raw time series data, performing a \emph{Z-Normalization} of the data can significantly improve distance computation results with the DTW metric\cite{mueen2016extracting}.

As a result, when working with raw time series representation, a \emph{Z-Normalization} of the data was conducted. The \emph{Z-Normalization} can be defined as:

$$X = \frac{X - MEAN(X)}{STD(X)} $$

\subsection{Dimensionality Reduction for Time Series}

Regarding the application of dimensionality reduction techniques to time series data, two popular algorithms of this nature were compared: \emph{PCA} and \emph{Stacked Autoencoders} (SAEs). The comparison was based on the rmse on reconstruction and on visual inspection of the reconstructed series.

In PCA the target number of features was selected as a function of the percentage of explained data variance. In this sense, principal components capable of representing $80\%$ of the data variance were selected, yielding a reduced dimensionality of 3 features.

SAEs were then trained for dimensionality reduction, targeting the same number of features as PCA. Different network architectures were considered while training the SAEs (that is, different number of autoencoders with different numbers of neurons per hidden layer, different activation functions, etc); however, in the best scenarios, SAEs were only capable of getting close to PCA in terms of reconstruction error, producing reconstructed time series considerably more distant to the original ones than PCA.

It is also worth mentioning at this point that both PCA and SAEs approaches were tested with normalised data, using the \emph{Min-Max} method. Figure \ref{day3_reconstruction} presents a reconstruction of water consumptions for one day using both a PCA decomposition and Stacked Autoencoders.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{images/pca_sae_day3.png}
	\caption{Reconstruction of water consumptions for one day using a PCA decomposition and a Stacked Autoencoder.}
	\label{day3_reconstruction}
\end{figure}

From the analysis of figure \ref{day3_reconstruction} it is clear that, even though SAEs may be able to reconstruct the original data with an error close to PCA, SAEs fail in capturing the shape and tendency of the series. As a proper learning could not be obtained with SAEs, PCA was the dimensionality reduction technique chosen to be applied in the experiments and exercises carried out in the remainder of this work.

\section{Clustering}

The choice of which clustering algorithm to apply in a given problem can strongly condition the obtained results. Several clustering algorithms have been proposed over the years and applied to a wide variety of problems.

It is common among authors to characterise and classify clustering approaches according to how the algorithms work. Aghabozorgi \emph{et al.}\cite{aghabozorgi2015time} proposes a general categorisation of clustering algorithms applied to time series featuring six different groups: Hierarchical, Partitioning, Model-based, Density-based, Grid-based and Multi-step clustering.

As mentioned in section \ref{methodology}, from the conducted literature review on time series clustering \emph{Hierarchical} and \emph{K-Means} clustering algorithms were identified as the most popular choice among researchers.

One of the main drawbacks of most time series clustering algorithms is related with the computational burden of finding similarities in the data being clustered. As the most robust similarity measures for time series rely on a raw representation, continuously searching for patterns based on similarities/dissimilarities in the data becomes very expensive in computation time.

Motivated by the fact that robust and less demanding alternatives to metrics such as the \emph{DTW} have not been achieved, some researchers studied ways of reducing this computational burden. This is the case of \emph{TADPole}\cite{begum2015accelerating}. Proposed in 2015 by Begum \emph{et al.}, TADPole exploits upper and lower bounds on DTW in a novel pruning strategy that avoids a large fraction of distance calculations, achieving results identical to brute force (but at least an order of magnitude faster).

In this sense, the mentioned cluster algorithms were selected for analysis at this point. Regarding HC and K-Means, as is often performed in the literature, both algorithms were combined\cite{jain2010data}. In the remainder of this section, details regarding the implementation and experimental setup of the two sets of algorithms being compared are covered.

\subsection{Combining Hierarchical and K-Means Clustering}

Formulating K-Means clustering into an algorithm comprises quite simple steps. Nevertheless, this simplicity does not avoid some of the most well-known and documented drawbacks of this algorithm: K-means requires the specification of a number of clusters prior to its execution; resulting from the previous point, K-means always groups the data into the specified number of clusters, meaning even if the number of clusters is inadequate to the dataset at hand, K-means will find such a number of partitions; even on perfect datasets it can get stuck on local minimum; and lastly, the final clustering solution is very sensitive to the initial cluster selection.

To avoid (or at least soften) these problems, an hybrid approach is typically used by combining \emph{hierarchical clustering} and \emph{k-means} methods. The procedure is as follows:

\begin{enumerate}
	\item Compute hierarchical clustering and cut the resulting dendrogram into k-clusters.
	
	\item Compute the centroid of each cluster.
	
	\item Compute K-means by using the previously computed centroids as the initial cluster selection.
\end{enumerate}

\subsection{Centroid Computation in K-Means}

Besides specifying the number of clusters, K-Means also allows for different methods to be applied when computing the centroid representative of a given cluster. A widely used method that can be applied to many distance metrics involves computing, for each dimension of the data, the average of all the samples assigned to the cluster in question. For this reason this method is often referred to as the \emph{average} method.

As mentioned by Aghabozorgi \emph{et al.}\cite{aghabozorgi2015time}, when working with time series data the average centroid computation method tends to only be applied when time series have equal length and a none-elastic distance metric (such as the Euclidean distance) is employed.

When time series are allowed to have different lengths, or when distance metrics of other natures are employed (namely the DTW) simply performing the mean of the time series at each point fails to capture the average shape of all the time series in the given cluster. As a result, alternative centroid computation methods more adequate to these distance metrics have been studied.

Probably the most common centroid computation method used with raw time series data is the \emph{medoid} method. According to this method, the centroid is the sample that minimizes the sum of squared distance to all the other samples within the cluster. Examples of the application of this method can be found in \cite{liao2006adaptive, izakian2015fuzzy}.

A more recent centroid computation method is the \emph{DTW Barycenter Averaging} (DBA)\cite{petitjean2011global}, proposed in 2011 by Petitjean \emph{et al.}. This method was specified for the DTW distance metric and, according to the authors of the cited work, is claimed to outperform all existing averaging techniques when applied to datasets of the UCR Archive\cite{keogh2006ucr}.

When computing the cluster centroid, DBA tries to minimize the sum of squared DTW distances from the average sequence (that is, the centroid) to all the sequences assigned to the cluster in question. A local optimisation strategy is implemented, which strongly depends on an initial guess for the average sequence - the initial centroid guess. As a result, improved results are usually obtained by performing multiple random initial starts. An example study making use of the DBA to compute the centroids for K-Means clustering can be found in \cite{petitjean2014dynamic}.

In short, taking into account that two distinct time series representation techniques are intended to be applied, different centroid computation methods were also implemented and the performances of the resulting clusterings compared:

\begin{itemize}
	\item Regarding the \emph{DTW} distance metric, the following centroid computation methods were considered: \emph{Average}, \emph{Medoid} and \emph{DBA}.
	
	\item With respect to the \emph{Euclidean} distance metric, only the \emph{Average} centroid method was considered.
\end{itemize}

\subsection{TADPole}

In the beginning of the current section a brief introduction to the TADPole clustering algorithm was performed, highlighting its main characteristics and motivations leading to its development. Regarding implementation details of this algorithm, two parameters must be defined prior to its applications: the \emph{window size} used in the DTW computations and a cutoff distance.

The TADPole algorithm makes use of a centred window when computing distances (using the DTW metric). For any observation $x_{i}$ the algorithm considers the points in the range $x_{i-w}$ and $x_{i+w}$ in the DTW computation. As such taking into account that the DTW is only applied for raw time series representation, and that such time series have a dimensionality of $24$, a window size of $23$ was defined.

The second parameter that needs to be defined is the cutoff distance, often represented as $d_{c}$. In an initial step of the algorithm, upper and lower bound on the DTW are used to find time series with many close neighbours. The mentioned cutoff distance is used as a threshold when determining a time series' neighbours: Any other time series whose distance is below $d_{c}$ is considered a neighbour. In the current work initial experiments were performed with different values for the $d_{c}$ threshold, obtaining a value of $1.5$ as the outcome. Nevertheless, further research on the most appropriate and adequate value for this parameter can still be performed.

Finally, regarding the cluster computation, TADPole takes the series that lie in dense areas (that is, the series that have many neighbours) as the cluster centroids.

\section{Summary of the Experiments}
\label{experiments_summary}

Following the discussion started in the previous sections, Table \ref{experiments_summary_table} presents a summary of the experiments performed, mentioning the algorithms and respective configurations adopted.

With respect to the last column of the table (entitled \emph{"Number of Clusters"}) an explanation for the selected ranges of values is performed on section \ref{initial_number_clusters}.

\begin{table}[ht]
	\centering
	\caption{Summary of the experiments performed.}
	\label{experiments_summary_table}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Data Type                                               & \begin{tabular}[c]{@{}c@{}}Distance\\ Metric\end{tabular} & \begin{tabular}[c]{@{}c@{}}Clustering\\ Algorithm\end{tabular} & \begin{tabular}[c]{@{}c@{}}Centroid\\ Computation\end{tabular}   & \begin{tabular}[c]{@{}c@{}}Number of\\ Clusters\end{tabular} \\ \hline
		Raw                                                     & DTW                                                       & \begin{tabular}[c]{@{}c@{}}HC\\ +\\ K-Means\end{tabular}       & \begin{tabular}[c]{@{}c@{}}Average;\\ DBA;\\ Medoid\end{tabular} & 2 to 8                                                       \\ \hline
		\begin{tabular}[c]{@{}c@{}}Reduced\\ (PCA)\end{tabular} & Euclidean                                                 & \begin{tabular}[c]{@{}c@{}}HC\\ +\\ K-Means\end{tabular}       & Average                                                          & 2 to 8                                                       \\ \hline
		Raw                                                     & DTW                                                       & TADPole                                                        & -                                                                & 2 to 8                                                       \\ \hline
	\end{tabular}
\end{table}


\section{Results Assessment}
\label{results_assessment}

The current section covers the distinct experiments conducted in the course of this work, presenting and discussing its main findings. As acknowledged in the previous section, different configurations for the clustering process were defined. Overall, the experimental procedure and consequent results analysis can be summarised in four main steps:

\begin{enumerate}
	\item Initially estimates for the number of clusters were obtained by performing \emph{Hierarchical Clustering} on the time series data and analysing the obtain dendrogram.
	
	\item Apply an hybrid clustering algorithm, where the centroids obtained for the different selected number of clusters in the previous point are used as initial centroids for the \emph{K-Means Clustering} algorithm.
	
	\item Apply cluster evaluation metrics to assess the quality of the clusters computed in the hybrid approach.
	
	\item Based on the cluster quality computed in the previous point select a subset of clustering results to be further analysed. Analysis conducted at this point focuses on descriptive characteristics of the categorisation performed, namely percentage of weekdays and weekend-days assigned to each cluster, percentage of days of each month assigned to each cluster, among others.
\end{enumerate}

In this sense, each of the four points mentioned in the previous listing will be covered in a separate subsection, within the remainder of the current section.

\subsection{Initial Number of Cluster Estimates}
\label{initial_number_clusters}

Based on the experimental setup presented in table \ref{experiments_summary_table}, and discussed throughout the document, time series' similarities were computed based on two distinct distance metrics: DTW and Euclidean. As such, hierarchical clustering using both metrics was computed to obtain the initial estimates for the number of clusters. Figures \ref{hierarchical_clustering_dtw} and \ref{hierarchical_clustering_euclidean} present the dendrograms obtained:

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.2]{images/Hierarchical_Clustering_DTW.png}
	\caption{Dendrogram obtained using the DTW metric.}
	\label{hierarchical_clustering_dtw}
\end{figure}

Starting with the analysis of Figure \ref{hierarchical_clustering_dtw}, referring to the hierarchical clustering of the z-normalised time series data, 2 major and uneven clusters can be identified and further analysed.

With respect to the right side of the dendrogram (represented in red) three main clusters appear to form. Further divisions can only be considered for significantly smaller distances, for which a high number of clusters needs to be considered. Analysing the left side of the dendrogram (represented in green) several divisions can be identified. For distances smaller than about 3 a high number of divisions can be identified.

Since water consumption patterns for an entire year are intended to be discovered with the goal of improving urban water management, the target number of clusters does not need to be considerably high: for example, if 300 clusters were formed it would be impossible to identify recurrent patterns of consumption.

In this sense, and considering the high number of divisions identified at the mentioned distance, a decision was made to keep it as the maximum number of clusters considered in this work. As the identified number of clusters for the mentioned distance is 8, the K-Means clustering algorithm will be applied to time series data using the DTW metric for a target number of clusters from 2 to 8. The reader's attention is drawn to Table \ref{experiments_summary_table}, where this range is presented.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.18]{images/Hierarchical_Clustering_Euclidean.png}
	\caption{Dendrogram obtained using the Euclidean metric.}
	\label{hierarchical_clustering_euclidean}
\end{figure}

Moving on to time series data represented using dimensionality reduction techniques the scenario is different from the one presented in Figure \ref{hierarchical_clustering_dtw}: even though two main clusters can still be identified, they are not as unbalanced as the ones in the previous analysis.

By using a reasoning similar to the previous analysis, for distances lower than 50 the number of clusters begins to increase rapidly, suggesting that distance to be defined as the limit for the number of clusters which, is this case, is 8. Similarly to the previous analysis, this value is presented in Table \ref{experiments_summary_table}.

Being in a completely unsupervised scenario alternative ranges for the number of clusters could be considered. Furthermore, a cyclic process could even be considered in which the dendrogram would be initially analysed to extract estimates for  the number of clusters, which would be computed using the K-Means algorithm. Upon further analysis of the results obtained with the K-Means the dendrogram could be analysed again, and new estimates for the value of $k$ could be computed, repeating this process. Nevertheless, and because of time restrictions, this process was only performed once.

\subsection{Initial Results Analysis}

Primeiro apresentar dendrogramas do hierarchical clustering obtido com cada uma das distance metrics. A partir daí apresentar ranges relativas ao número de clusters que testámos.

Referir quais as métricas que utilizámos para avaliar os resultados: Apresentar métricas num itemize e fazer uma breve descrição, colocar fórmulas, dizer que valores são bons, etc

Fazer uma reflexão sobre os resultados e dizer quais aqueles que merecem ser mais estudados.

Atenção que analisei muita coisa com base no SC mas este pode nao ser o melhor método... Combinar um pouco a análise com o elbow method! (SC e um indicador normalisado, o que tem vantagens porque nao e tao sujeito a subjectividade como e o elbow method, mas por vezes em dados reais pode ser necessario experiencia e analise visual que o SC nao consegue traduzir)

\textbf{FIXME: Não esquecer de falar do facto de o SC poder nao ser a metrica mais adequada para avaliar os clusters!}

\subsection{Further Results Analysis}

Apresentar novas métricas, resultados e reflexão

Meter gráficos aqui

\section{Conclusion}
\label{conclusions}

De uma forma geral o numero de clusters parece andar proximo de 4, pelo menos andamos sempre a volta desse valor...


\bibliographystyle{ieeetr}
\bibliography{bibliography.bib}

\end{document}